{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f4fad45c-b237-4a23-87c3-5e8470be6cf8",
      "metadata": {
        "id": "f4fad45c-b237-4a23-87c3-5e8470be6cf8"
      },
      "outputs": [],
      "source": [
        "# this is for small testing\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd1f536d-643b-4497-8399-99e9e80a5b41",
      "metadata": {
        "id": "bd1f536d-643b-4497-8399-99e9e80a5b41"
      },
      "outputs": [],
      "source": [
        "train_file_path = 'train_file_path'  # Replace with your train file path\n",
        "train_data = pd.read_csv(train_file_path, nrows=20000)\n",
        "\n",
        "# Check the shape to confirm\n",
        "print(train_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab584fb-5d08-4250-a9d3-1cd943a958ea",
      "metadata": {
        "id": "3ab584fb-5d08-4250-a9d3-1cd943a958ea"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
        "# Define text cleaning function\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text\n",
        "    return None\n",
        "\n",
        "# Apply text cleaning\n",
        "train_data['cleaned_review'] = train_data['review_body'].apply(clean_text)\n",
        "train_data = train_data.dropna(subset=['cleaned_review'])\n",
        "\n",
        "# Tokenize and encode data\n",
        "max_length = 128\n",
        "\n",
        "def encode_review(text):\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for review in train_data['cleaned_review']:\n",
        "    encoded_review = encode_review(review)\n",
        "    input_ids.append(encoded_review['input_ids'])\n",
        "    attention_masks.append(encoded_review['attention_mask'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "# Use existing sentiment labels\n",
        "labels = torch.tensor(train_data['star_rating'].values)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
        "    input_ids, attention_masks, labels, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 16\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fac04fb-c7b8-419f-9d9f-6e87c71290a2",
      "metadata": {
        "id": "3fac04fb-c7b8-419f-9d9f-6e87c71290a2"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, get_scheduler\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-large-cased', num_labels=2)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "epochs = 3\n",
        "num_training_steps = epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d69613b-f0d2-4b06-b848-ec8dde5a65c0",
      "metadata": {
        "id": "4d69613b-f0d2-4b06-b848-ec8dde5a65c0"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch_input_ids, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
        "        outputs = model(batch_input_ids, attention_mask=batch_masks, labels=batch_labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} completed. Loss: {loss.item()}\")\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained('./fine_tuned_bert_sentiment_model')\n",
        "tokenizer.save_pretrained('./fine_tuned_bert_sentiment_model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress the model folder into a ZIP file\n",
        "import shutil\n",
        "shutil.make_archive('bert_sentiment_model', 'zip', './fine_tuned_bert_sentiment_model')"
      ],
      "metadata": {
        "id": "HV5J46tcJ_dr"
      },
      "id": "HV5J46tcJ_dr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "93b6eed4-5e8b-4e4d-a836-ec3cbcf6760c",
      "metadata": {
        "id": "93b6eed4-5e8b-4e4d-a836-ec3cbcf6760c"
      },
      "outputs": [],
      "source": [
        "# Load test data\n",
        "test_file_path = 'test_file_path'  # Replace with your test file path\n",
        "test_data = pd.read_csv(test_file_path, nrows= 20000) #nrows is to shrink no. of rows to 20k\n",
        "\n",
        "# Apply text cleaning\n",
        "test_data['cleaned_review'] = test_data['review_body'].apply(clean_text)\n",
        "test_data = test_data.dropna(subset=['cleaned_review'])\n",
        "\n",
        "# Tokenize and encode data\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for review in test_data['cleaned_review']:\n",
        "    encoded_review = encode_review(review)\n",
        "    input_ids.append(encoded_review['input_ids'])\n",
        "    attention_masks.append(encoded_review['attention_mask'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "# Use existing sentiment labels\n",
        "labels = torch.tensor(test_data['star_rating'].values)\n",
        "\n",
        "# Create DataLoader\n",
        "test_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c87a7375-8cd9-4df3-b4d1-e4dc5bf06f56",
      "metadata": {
        "id": "c87a7375-8cd9-4df3-b4d1-e4dc5bf06f56"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained('./fine_tuned_bert_sentiment_model')\n",
        "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_bert_sentiment_model')\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch_input_ids, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch_input_ids, attention_mask=batch_masks)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    labels = batch_labels.cpu().numpy()\n",
        "\n",
        "    all_preds.extend(preds)\n",
        "    all_labels.extend(labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "# Calculate metrics for binary classification\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='binary')\n",
        "recall = recall_score(all_labels, all_preds, average='binary')\n",
        "f1 = f1_score(all_labels, all_preds, average='binary')\n",
        "\n",
        "# Print the metrics in one line\n",
        "print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}