{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b8f01a2-8d99-4c57-a926-091fe0a63ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\rupam\\anaconda3\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in c:\\users\\rupam\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\rupam\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rupam\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e206af6-801b-442c-8769-fa85d9b492c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rupam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "    star_rating                                        review_body\n",
      "0            1  As a family allergic to wheat, dairy, eggs, nu...\n",
      "1            1  My favorite nut.  Creamy, crunchy, salty, and ...\n",
      "2            1  This green tea tastes so good! My girlfriend l...\n",
      "3            1  I love Melissa's brand but this is a great sec...\n",
      "4            1                                               good\n",
      "Test Data:\n",
      "    star_rating                                        review_body\n",
      "0            1  I loved it and I wish there was a season 3... ...\n",
      "1            1  As always it seems that the best shows come fr...\n",
      "2            1  This movie isn't perfect, but it gets a lot of...\n",
      "3            1                excellant this is what tv should be\n",
      "4            1  Brilliant film from beginning to end. All of t...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "# Step 2: Import Libraries and Load the Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the train and test datasets from separate CSV files\n",
    "train_file_path = 'csv files/Kitchen1.csv'  # Replace with the path to your training CSV file\n",
    "test_file_path = 'csv files/DVD11.csv'    # Replace with the path to your testing CSV file\n",
    "\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# Display the first few rows of train and test data\n",
    "print(\"Training Data:\\n\", train_data.head())\n",
    "print(\"Test Data:\\n\", test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2660802-f717-4524-beb5-4876c1cdd526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rupam\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample train input IDs: tensor([  101,  2004,  1037,  2155, 27395,  2000, 10500, 11825,  6763, 12264,\n",
      "         1998,  2195,  2060,  2477,  2057,  2293,  1996,  2972, 26369,  2015,\n",
      "         2173,  2240,  1997,  3688,  2004,  2009,  4473,  2149,  2000,  8670,\n",
      "         3489, 18452,  2007, 10124,  3947,  1998, 12760,  2087,  2035, 24395,\n",
      "        23301,  1998,  1043,  7630,  6528, 23301, 21109,  2788,  2074, 18168,\n",
      "         4183,  2028,  2030,  2048,  2035,  2121, 21230,  2012,  2087,  2061,\n",
      "         2049,  2307,  2000,  2156,  1037,  4666,  2580,  2302,  2116,  1997,\n",
      "         1996,  2087,  2691,  2035,  2121, 21230,  3602,  2122,  2145,  2031,\n",
      "        25176,  1998,  9781,  2057, 16678,  2122,  2006,  1037,  3180,  3978,\n",
      "         1998,  2031,  2042,  2725,  2061,  2005,  2086,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Sample train attention mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Sample train label: tensor(1)\n",
      "Sample test input IDs: tensor([ 101, 1045, 3866, 2009, 1998, 1045, 4299, 2045, 2001, 1037, 2161, 1017,\n",
      "        1045, 3427, 2161, 1016, 1998, 3866, 2008, 2004, 2092,  102,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Sample test attention mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Sample test label: tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Data Preprocessing (Text Cleaning, Tokenization, Padding)\n",
    "import torch\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "        return text\n",
    "    return None  # Return None for non-string inputs\n",
    "\n",
    "# Apply text cleaning and drop rows with None values for training data\n",
    "train_data['cleaned_review'] = train_data['review_body'].apply(clean_text)\n",
    "train_data = train_data.dropna(subset=['cleaned_review'])  # Drop rows where 'cleaned_review' is None\n",
    "\n",
    "# Apply text cleaning and drop rows with None values for test data\n",
    "test_data['cleaned_review'] = test_data['review_body'].apply(clean_text)\n",
    "test_data = test_data.dropna(subset=['cleaned_review'])  # Drop rows where 'cleaned_review' is None\n",
    "\n",
    "# Tokenize the reviews using BERT tokenizer\n",
    "\n",
    "def encode_review(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text, \n",
    "        add_special_tokens=True, \n",
    "        max_length=max_length, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_attention_mask=True, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the training dataset\n",
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "\n",
    "for review in train_data['cleaned_review']:\n",
    "    encoded_review = encode_review(review)\n",
    "    train_input_ids.append(encoded_review['input_ids'])\n",
    "    train_attention_masks.append(encoded_review['attention_mask'])\n",
    "\n",
    "# Apply tokenization to the test dataset\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "for review in test_data['cleaned_review']:\n",
    "    encoded_review = encode_review(review)\n",
    "    test_input_ids.append(encoded_review['input_ids'])\n",
    "    test_attention_masks.append(encoded_review['attention_mask'])\n",
    "\n",
    "# Convert lists to tensors for training and test datasets\n",
    "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim=0)\n",
    "\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "\n",
    "# Use the existing sentiment labels (0 or 1) directly from the CSV file\n",
    "train_labels = torch.tensor(train_data['star_rating'].values)\n",
    "test_labels = torch.tensor(test_data['star_rating'].values)\n",
    "\n",
    "# Print a few samples\n",
    "print(\"Sample train input IDs:\", train_input_ids[0])\n",
    "print(\"Sample train attention mask:\", train_attention_masks[0])\n",
    "print(\"Sample train label:\", train_labels[0])\n",
    "\n",
    "print(\"Sample test input IDs:\", test_input_ids[0])\n",
    "print(\"Sample test attention mask:\", test_attention_masks[0])\n",
    "print(\"Sample test label:\", test_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6bb4916-ba15-47de-9979-10bda7fad850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 204687\n",
      "Test data size: 12446\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Prepare Data for Training\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create PyTorch DataLoader for training and test sets\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Training set\n",
    "train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Test set\n",
    "test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Print to confirm successful creation of dataloaders\n",
    "print(f\"Training data size: {len(train_dataloader.dataset)}\")\n",
    "print(f\"Test data size: {len(test_dataloader.dataset)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aab950d1-be15-4d41-bee2-2d2bdfa4b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is running on CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rupam\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Load BERT Model and Set Up Optimizer\n",
    "from transformers import BertForSequenceClassification, AdamW, get_scheduler\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Print whether the program is executing on CPU or GPU\n",
    "if device.type == 'cuda':\n",
    "    print(\"Model is running on GPU\")\n",
    "else:\n",
    "    print(\"Model is running on CPU\")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "epochs = 3\n",
    "num_training_steps = epochs * len(train_dataloader)  # Make sure train_dataloader is defined earlier\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f75b765-f109-47f7-be18-5f8e880e5b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404452f2d48d49df8b47bf0b6184e236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_input_ids, attention_mask\u001b[38;5;241m=\u001b[39mbatch_masks, labels\u001b[38;5;241m=\u001b[39mbatch_labels)\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Optimizer and scheduler step\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 6: Training the BERT Model\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize the progress bar for tracking training progress\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()  # Set model to training mode\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0  # Track loss for the epoch\n",
    "    for batch in train_dataloader:\n",
    "        batch_input_ids, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_masks, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()  # Accumulate loss for the epoch\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer and scheduler step\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed. Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454b3e7-1074-4248-baa2-177e24b50ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate the Model on the Validation Set\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    batch_input_ids, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
    "    \n",
    "    # No gradient calculation during evaluation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_masks)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    labels = batch_labels.cpu().numpy()\n",
    "    \n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e0a2c-0e0a-44bb-9823-6523d55f8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Save the Model (Optional)\n",
    "# Save the fine-tuned model for future use\n",
    "model.save_pretrained('./fine_tuned_bert_sentiment_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_bert_sentiment_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
