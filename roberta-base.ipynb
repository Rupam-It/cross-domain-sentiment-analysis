{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16629a6-87a9-4ddd-a45b-692717a0da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for small testing\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd234d3e-699b-4aa3-8112-95221ef4af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = '/content/DVD11.csv'  # Replace with your train file path\n",
    "train_data = pd.read_csv(train_file_path, nrows=20000)\n",
    "\n",
    "# Check the shape to confirm\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d479b-7ce0-4712-857f-a03142e1a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text\n",
    "    return None\n",
    "\n",
    "# Apply text cleaning\n",
    "train_data['cleaned_review'] = train_data['review_body'].apply(clean_text)\n",
    "train_data = train_data.dropna(subset=['cleaned_review'])\n",
    "\n",
    "# Tokenize and encode data\n",
    "max_length = 128\n",
    "\n",
    "def encode_review(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for review in train_data['cleaned_review']:\n",
    "    encoded_review = encode_review(review)\n",
    "    input_ids.append(encoded_review['input_ids'])\n",
    "    attention_masks.append(encoded_review['attention_mask'])\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Use existing sentiment labels\n",
    "labels = torch.tensor(train_data['star_rating'].values)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a110539-48cd-428b-87bd-fd618fd56bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AdamW, get_scheduler\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "from transformers import RobertaForSequenceClassification\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 3\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a2ca83-a9c2-4a97-9f31-1dc6bb1e5ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch_input_ids, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_masks, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed. Loss: {loss.item()}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./fine_tuned_bert_sentiment_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_bert_sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0a637-fbea-4804-bd4a-b6471b2b5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer locally\n",
    "model.save_pretrained('./fine_tuned_bert_sentiment_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_bert_sentiment_model')\n",
    "\n",
    "# Compress the model folder into a ZIP file\n",
    "import shutil\n",
    "shutil.make_archive('bert_uncased_DVD', 'zip', './fine_tuned_bert_sentiment_model')\n",
    "\n",
    "# Download the ZIP file to your local machine\n",
    "from google.colab import files\n",
    "files.download('bert_uncased_DVD.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7a7f3-8844-43ea-887b-8fe58cad1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_file_path = '/content/DVD11.csv'  # Replace with your test file path\n",
    "test_data = pd.read_csv(test_file_path,nrows=20000)\n",
    "\n",
    "# Apply text cleaning\n",
    "test_data['cleaned_review'] = test_data['review_body'].apply(clean_text)\n",
    "test_data = test_data.dropna(subset=['cleaned_review'])\n",
    "\n",
    "# Tokenize and encode data\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for review in test_data['cleaned_review']:\n",
    "    encoded_review = encode_review(review)\n",
    "    input_ids.append(encoded_review['input_ids'])\n",
    "    attention_masks.append(encoded_review['attention_mask'])\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Use existing sentiment labels\n",
    "labels = torch.tensor(test_data['star_rating'].values)\n",
    "\n",
    "# Create DataLoader\n",
    "test_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a24e1-0f00-4243-b0cd-c8ed8e4f146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Load the saved RoBERTa model and tokenizer\n",
    "model = RobertaForSequenceClassification.from_pretrained('./fine_tuned_roberta_sentiment_model')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./fine_tuned_roberta_sentiment_model')\n",
    "\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch_input_ids, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_masks)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    labels = batch_labels.cpu().numpy()\n",
    "\n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "# Calculate metrics for binary classification\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='binary')\n",
    "recall = recall_score(all_labels, all_preds, average='binary')\n",
    "f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "# Print the metrics in one line\n",
    "print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
