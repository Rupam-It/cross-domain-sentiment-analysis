{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in domain test\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/content/DVD11.csv')\n",
    "\n",
    "# Preprocess the text and labels\n",
    "# Ensure 'review_body' is a string and 'star_rating' exists\n",
    "df = df[df['review_body'].apply(lambda x: isinstance(x, str))]  # Keep only rows with string text\n",
    "texts = df['review_body'].tolist()\n",
    "labels = df['star_rating'].tolist()  # Assuming star_rating is your label\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Generate embeddings for each review\n",
    "X = [get_bert_embeddings(text) for text in texts]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after domain adoptation \n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(texts):\n",
    "    \"\"\"Converts input text list into BERT embeddings.\"\"\"\n",
    "    all_embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Move the output back to CPU before converting to numpy\n",
    "        all_embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy())\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Load book reviews dataset\n",
    "books_df = pd.read_csv('/content/books_reviews.csv')\n",
    "\n",
    "# Preprocess the text and labels (ensure only rows with valid text data are kept)\n",
    "books_df = books_df[books_df['review_body'].apply(lambda x: isinstance(x, str))]\n",
    "books_texts = books_df['review_body'].tolist()\n",
    "books_labels = books_df['star_rating'].tolist()\n",
    "\n",
    "# Generate BERT embeddings for book reviews\n",
    "books_X = get_bert_embeddings(books_texts)\n",
    "\n",
    "# Split book data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(books_X, books_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define domain-invariant feature learning model\n",
    "class DomainInvariantModel(torch.nn.Module):\n",
    "    def __init__(self, bert_embedding_dim, num_classes):\n",
    "        super(DomainInvariantModel, self).__init__()\n",
    "        self.sentiment_classifier = torch.nn.Linear(bert_embedding_dim, num_classes)\n",
    "        self.domain_classifier = torch.nn.Linear(bert_embedding_dim, 2)  # 2 classes: book and electronics\n",
    "\n",
    "    def forward(self, x):\n",
    "        sentiment_logits = self.sentiment_classifier(x)\n",
    "        domain_logits = self.domain_classifier(x)\n",
    "        return sentiment_logits, domain_logits\n",
    "\n",
    "# Initialize domain-invariant feature learning model\n",
    "domain_invariant_model = DomainInvariantModel(bert_embedding_dim=768, num_classes=5).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(domain_invariant_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Train domain-invariant feature learning model\n",
    "for epoch in range(3):\n",
    "    for batch in DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=32, shuffle=True):\n",
    "        input_ids, labels = batch\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        sentiment_logits, domain_logits = domain_invariant_model(input_ids)\n",
    "        sentiment_loss = criterion(sentiment_logits, labels)\n",
    "        domain_loss = criterion(domain_logits, torch.zeros_like(labels))  # assume all book reviews are from the book domain\n",
    "        total_loss = sentiment_loss + domain_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    domain_invariant_model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        for batch in DataLoader(TensorDataset(torch.tensor(X_test), torch.tensor(y_test)), batch_size=32, shuffle=False):\n",
    "            input_ids, labels = batch\n",
    "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "            \n",
    "            sentiment_logits, _ = domain_invariant_model(input_ids)\n",
    "            _, predicted = torch.max(sentiment_logits, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = total_correct / len(X_test)\n",
    "        print(f'Epoch {epoch+1}, Val Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Load electronics reviews dataset\n",
    "electronics_df = pd.read_csv('/content/electronics_reviews.csv')\n",
    "\n",
    "# Preprocess electronics reviews\n",
    "electronics_df = electronics_df[electronics_df['review_body'].apply(lambda x: isinstance(x, str))]\n",
    "electronics_texts = electronics_df['review_body'].tolist()\n",
    "electronics_labels = electronics_df['star_rating'].tolist()\n",
    "\n",
    "# Generate BERT embeddings for electronics reviews\n",
    "electronics_X = get_bert_embeddings(electronics_texts)\n",
    "\n",
    "# Make predictions on electronics reviews using the trained model\n",
    "domain_invariant_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for batch in DataLoader(TensorDataset(torch.tensor(electronics_X)), batch_size=32, shuffle=False):\n",
    "        input_ids = batch[0].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        sentiment_logits, _ = domain_invariant_model(input_ids)\n",
    "        _, predicted = torch.max(sentiment_logits, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Evaluate performance on out-of-domain (electronics reviews) data\n",
    "electronics_accuracy = accuracy_score(electronics_labels, predictions)\n",
    "electronics_f1 = f1_score(electronics_labels, predictions, average='weighted')\n",
    "electronics_precision = precision_score(electronics_labels, predictions, average='weighted')\n",
    "electronics_recall = recall_score(electronics_labels, predictions, average='weighted')\n",
    "\n",
    "# Print all evaluation metrics\n",
    "print(f\"Domain Adaptation Performance (Books -> Electronics):\")\n",
    "print(f\"Accuracy: {electronics_accuracy}\")\n",
    "print(f\"F1 Score: {electronics_f1}\")\n",
    "print(f\"Precision: {electronics_precision}\")\n",
    "print(f\"Recall: {electronics_recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
